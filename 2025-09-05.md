---
id: 2025-09-05
aliases: []
tags:
  - daily-notes
---

# WEPSKAM

## 1 - Introduction
## 2 - Commodity Hardware Today
## 3 - CPU caches ‼️

CPU's today are orders of magnitude faster than those 25 years ago.
Back then the *frequency* of a *CPU core* was *similar* to that of the *memory bus*
> In the 90's things changed as
> - `CPU` designers *increased* frequency.
> - frequency of `memory` bus *did not increase proportionally*.
>   This is *not because* it was *not possible to do so*; but because it was **very expensive to do** it.
>
>> [!info]
>> RAM as fast as the current CPU cores is far more expensive than any slower dynamic RAM

#working-set

32GB RAM
```
_______________
|             | -> 0x BEEFDEAD DEADBEEF
|             |
|             |
|-------------|
|working-set  | --> occupied by a program (like neovim)
|-------------|
|             |
|             |
|             |
|_____________| -> 0x 00000000 00000001
```
If there are 2 options
1. Small, very fast RAM
2. Large, comparatively fast RAM
The second will always win because we need to account for swap memory as well. i.e. If the #working-set for a program exceeds the size of RAM, some "pages" will swapped to disk.
Every time something is needed which is not in the RAM, it will fetch from disk (which is again, slower).
Which means the second system which has large size of RAM will win because it can hold the entire #working-set in memory which means it avoids any disk access.

> [!hint]
> However this does not mean that all of the programs layout has to fit in the RAM or there is nothing we can do.
> In fact, a computer can have small smount of SRAM along with large amount of DRAM.
> One implementation can be to dedicate certain processor address space to SRAM and the rest to the DRAM. The OS would then take care of optimally distibuting data between the two. But htis is so much overhead that it nullifies the point of having the fast SRAM available in the first place.
> Another implementation can be make it so that instead of putting the SRAM under the control of the OS or the user, it will be directly administered by the CPU. IN this mode, the SRAM is allowed to make temporary copies of data(which is likely to be used soon by CPU) in the main memory. In other words the SRAM will cache the most likely used data.
>> [!tip]
>> Identifying this most likely used data is possible because temporal and spatial locality.
>> TODO: Explain temporal and spatial locality more.

[[Drawing 2025-09-09 12.01.45.excalidraw]]

## 4 - Virtual Memory ‼️

- Each process gets provided certain virtual adress spaces.
- These virtual adress spaces are implemented by the Virtual Memory Subsystem of the processor.
- This causes each process to think that they are in their own virtual world i.e. they are the only ones running on the syste i.e. they are the only ones running on the system.

Refer to other resources for the advantages and disadvantages of the virtual memory implementation.
This section instead goes deep into the actual *implementation detail of virtual memory* and it's *associated costs*.

- Virtual address spaces are implemented by the MMU unit of the CPU.
- ==OS then has to fill out== the *page table* **data structures**.
    - The best way to understand is to introduce these ==data structures== (which are used to describe the virtual address space).
- The rest is done by the CPU.

- The input to the address translation done by the MMU is a virtual address.
> Usually no restrictions on what this input value will be.
> - It is 32 bit on 32 bit systems; 64 bit on 64 bit systems and so on.
> - Some architectures use an indirection which is called a segment
> - This input address generation involves adding offsets to every logical address (Think of turning the address access into array access, so that we can "index" into a segment)
> - This **address generation** can be *ignored* for now as it is not something programmers have to care about wrt performance handling.

### Simplest address translation
The **address translation** of *virtual address* into *physical address* is the more interesting part.
- The MMU remaps addresses on a page by page basis.
- Similar to cache line addressing, virtual address is split into parts.
- These parts are used to index into various tables (which is one of the above mentioned data structures)
- All parts are then combined to construct the final physical address.

For the simplest model, we have only one 1 level of tables.
> Where the virtual address is split into Directory and Offset.
> The Directory is used to find an entry in the page directory table.
> This entry determines the address of a physical memory page; and more than one entry can point to the same page.
> Then the Offset is used to index into that page to determine the final address of the memory cell, which is mapped to this virtual address.
> The entry also contains some more information about access permissions.

This page directory table is stored in main memory, and a pointer to the base address is stored in a special CPU register.
> The OS allocates contiguous physical memory for this. The appropriate bits from the virtual address are then used to index into this table, which is an array of entries.

- [ ] Fill out example for 32 bit system

### Multi level page tables
### Optimizing page table access

## 5 - NUMA


## 6 - What programmers can do ‼️‼️


## 7 - Tools to use for help


## 8 - What lies in the future


___
